{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, click run all to run this code\n",
    "\n",
    "2, it take 1~2 hour to finish "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to find spark path, please change path to your spark install path\n",
    "try:\n",
    "    import findspark\n",
    "    findspark.init('/home/mark/spark-2.2.1-bin-hadoop2.7/')\n",
    "except:\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import length,rand\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : find dog/cat owner\n",
    "\n",
    "' The criterion of filtering dog or cat owners is that their comments containing word cat or dog. This assumption is not \n",
    "rigorious since the user may just metioned cat or dog by chance. There are two reason i choose this criterion: \n",
    "1, It give fairly amount of cat/dog owner, about 1/11 total of data. \n",
    "2, For business intuition, there is a high probability that majority of these users are interested on topic related with cat or dog. \n",
    "\n",
    "   A more reasonable approach should using a iterative approach. For each interation, using set of key word from classifier of dog/cat owner as criterion to pick up training data labled as cat/dog until the set of key words became stable. ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('animal').config('spark.master','local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spark.read.csv('animals_comments.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 5820035 comments.\n"
     ]
    }
   ],
   "source": [
    "print('There are total {} comments.'.format(data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+\n",
      "|        creator_name|userid|             comment|\n",
      "+--------------------+------+--------------------+\n",
      "|        Doug The Pug|  87.0|I shared this to ...|\n",
      "|        Doug The Pug|  87.0|  Super cute  üòÄüêïüê∂|\n",
      "|         bulletproof| 530.0|stop saying get e...|\n",
      "|       Meu Zool√≥gico| 670.0|Tenho uma jiboia ...|\n",
      "|              ojatro|1031.0|I wanna see what ...|\n",
      "|     Tingle Triggers|1212.0|Well shit now Im ...|\n",
      "|Hope For Paws - O...|1806.0|when I saw the en...|\n",
      "|Hope For Paws - O...|2036.0|Holy crap. That i...|\n",
      "|          Life Story|2637.0|Ê≠¶Âô®„ÅØ„ÇØ„Ç®„Çπ„Éà„ÅßË≤∞„Åà„Çã„Çì„Åò„ÇÉ„Å™„ÅÑ„Çì...|\n",
      "|       Brian Barczyk|2698.0|Call the teddy Larry|\n",
      "|            The Dodo|2702.0|  üòêü§îüòìüò¢üò≠üò≠üò≠üò≠üòü|\n",
      "|Hope For Paws - O...|2911.0|That mother cat l...|\n",
      "|Hope For Paws - O...|2911.0|Its people like H...|\n",
      "|   Talking Kitty Cat|2911.0|steve: No wet foo...|\n",
      "|    Brave Wilderness|3224.0|Dont call this a ...|\n",
      "|          MaxluvsMya|3267.0|why are you alway...|\n",
      "|Rise Up Society F...|3372.0|          Deb Tucker|\n",
      "|            The Dodo|3466.0|Thats a deer isnt...|\n",
      "|    Brave Wilderness|3466.0|there is no safe ...|\n",
      "|    Brave Wilderness|3466.0|Red before yellow...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to rdd \n",
    "datardd=data.rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out data without creator_name or userid or comment\n",
    "\n",
    "# data without creator name or userid or comment\n",
    "df_anomal=datardd.filter(lambda x: (x.creator_name==None)or (x.userid==None)or (x.comment==None))\n",
    "\n",
    "# normal data with creator_name and userid and comment \n",
    "df_normal=datardd.filter(lambda x:  (x.creator_name!=None) and (x.userid!=None) and (x.comment!=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intution: sometimes, one used may be post several comments under same creator_name channel. \n",
    "# so i conbined the comment of same user under same channel with reduce key function, key-pair is ( userid, comments)\n",
    "# x[1] is userid   x[2] is comment\n",
    "\n",
    "df_combine=df_normal.map(lambda x: ((x[1]),x[2])).   \\\n",
    "                     reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2524849"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combine.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_combine.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cate(arr):\n",
    "    ''' use key word select cat/dog owner\n",
    "    assumption: The cat/dog owner has keyword [cat, dog] on their comment\n",
    "    '''\n",
    "    \n",
    "    tem={}\n",
    "#    tem['creator_name']=arr[0][0]\n",
    "#    tem['userid']=arr[0][1]\n",
    "    tem['userid']=arr[0]\n",
    "    \n",
    "    comment=arr[1].strip().lower().split()     # cleaning comment data \n",
    "    clean_comment=[ x for x in comment if x.isalpha()]  # remove digit, puncuation \n",
    "    \n",
    "    tem['combined_comment']=(' ').join(clean_comment)\n",
    "    if 'dog' in (tem['combined_comment']):\n",
    "        tem['category']='cat_dog'\n",
    "        return tem\n",
    "    elif 'cat' in (tem['combined_comment']):\n",
    "        tem['category']='cat_dog'\n",
    "        return tem\n",
    "    else:\n",
    "        tem['category']='other'\n",
    "        return tem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use add_cate function to add label [cat_dog, other] on data and clean_comment \n",
    "\n",
    "df_cate=df_combine.map(lambda x:Row(**add_cate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tem=df_combine.map(lambda x:Row(**add_cate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 2524849 userid now.\n"
     ]
    }
   ],
   "source": [
    "print('There are total {} userid now.'.format(df_cate.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert rdd to dataframe \n",
    "new_frame=df_cate.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------+\n",
      "|category|    combined_comment|   userid|\n",
      "+--------+--------------------+---------+\n",
      "|   other|subscribed he is ...|2097155.0|\n",
      "|   other|noooooooooooooooo...|1048580.0|\n",
      "|   other|the christmas mus...|      5.0|\n",
      "| cat_dog|what do you do wi...|2097160.0|\n",
      "|   other|      –≤—ã –ª—É—á—å—à–µ –≤—Å–µ—Ö|1048585.0|\n",
      "|   other|       i hope hes ok|     10.0|\n",
      "|   other|made me go rewatc...|2097165.0|\n",
      "|   other|how many times di...|1048590.0|\n",
      "|   other|i guess im a hedg...|     15.0|\n",
      "|   other|this channel is s...|2097170.0|\n",
      "|   other|people are heartless|1048595.0|\n",
      "|   other|explains my depre...|     20.0|\n",
      "|   other|at you can see a ...|2097175.0|\n",
      "| cat_dog|he speaks german ...|1048600.0|\n",
      "|   other|                    |     25.0|\n",
      "|   other|its never gonna h...|2097180.0|\n",
      "|   other| use your name brian|1048605.0|\n",
      "|   other|                    |     30.0|\n",
      "|   other|    what gun are you|2097185.0|\n",
      "|   other|my dick hurts whi...|1048610.0|\n",
      "+--------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# each userid only have one combine_comment \n",
    "new_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment:\n",
    " Now, each userid only have feature as combined_comments,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: build classifier\n",
    "\n",
    "' Build classifier: \n",
    "  data_cleaning_process (add_cate function): strip blank space, convert all words into lower, remove all none_letter,\n",
    "  add new feature: length of comments. The intution behind this is that ususally the comments of the dog/cat owner have more word on comment.\n",
    "  using Logistic classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+------------------+\n",
      "|category|      avg(userid)|       avg(length)|\n",
      "+--------+-----------------+------------------+\n",
      "|   other|1271203.735342072|101.08419325590432|\n",
      "| cat_dog|1269302.992542195| 522.0088307599414|\n",
      "+--------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# intuition: ususally the comments of the dog/cat owner have more word on comment.\n",
    "# so create a new feather as length of comment\n",
    "\n",
    "new_frame=new_frame.withColumn('length',length(new_frame['combined_comment']))\n",
    "new_frame.groupby('category').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame with label cat_dog\n",
    "df_dog_cat=new_frame.filter((new_frame['category']=='cat_dog') )\n",
    "# data_frame with label other\n",
    "other=new_frame.filter((new_frame['category']=='other') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 278232 userid could be dog/cat owner.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} userid could be dog/cat owner.'.format(df_dog_cat.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the data is extremly unbalanced, so select 1:1 ratio of labeled dog_cat and other data\n",
    "# because there are total 3176574 comments, and \n",
    "# about 319996 related with cat/dog. The ratio is roughly 10:1\n",
    "# so random split rest (data not labeled cat_dog) into 9:1 ratio, \n",
    "# and combine those 10% with data labeled as cat_dog  to build training data \n",
    "\n",
    "unlabeled9,unlabeled1=other.randomSplit([0.7,0.3])\n",
    "\n",
    "data=df_dog_cat.unionAll(unlabeled1).orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF,Tokenizer,IDF,VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer,StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.sql import Column\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.registerTempTable('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine creator and comment to build a new text feature \n",
    "\n",
    "#new_dog_cat=spark.sql(\"select *, \\\n",
    "#                  CONCAT(creator_name,' ',comment) as cnc from data\")\n",
    "new_dog_cat=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the category\n",
    "encode=StringIndexer(inputCol='category',outputCol='label')\n",
    "# tokenization \n",
    "token_cnc=Tokenizer(inputCol='combined_comment',outputCol='cnc_token')\n",
    "# remove stopwords\n",
    "stopremover=StopWordsRemover(inputCol='cnc_token',outputCol='stop_remove')\n",
    "# build term frenquence \n",
    "cnc_TF=HashingTF(inputCol='stop_remove',outputCol='cnc_tf')\n",
    "# build term frenquence inverse document frequence\n",
    "idf=IDF(inputCol='cnc_tf',outputCol='cnc_tfidf')\n",
    "\n",
    "vectorass=VectorAssembler(inputCols=['cnc_tfidf','length'],outputCol='features')\n",
    "\n",
    "# build pipeline for data processisng \n",
    "processing_pipeline=Pipeline(stages=[encode,token_cnc,stopremover,cnc_TF,idf,vectorass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# used build pipeline processing data \n",
    "processor=processing_pipeline.fit(new_dog_cat)\n",
    "\n",
    "processed_data=processor.transform(new_dog_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data=processed_data.select('label','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=processed_data.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[category: string, combined_comment: string, userid: double, length: int, label: double, cnc_token: array<string>, stop_remove: array<string>, cnc_tf: vector, cnc_tfidf: vector, features: vector]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.select('category','label').distinct().rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_encode={}\n",
    "for i in range(len(b)):\n",
    "    dict_encode[b[i].category]=b[i].label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test part\n",
    "train,test=clean_data.randomSplit([0.7,0.3])\n",
    "\n",
    "# build classifier\n",
    "#nb=LogisticRegression(featuresCol='features')\n",
    "nb=NaiveBayes(featuresCol='features')\n",
    "\n",
    "\n",
    "clf=nb.fit(train)\n",
    "\n",
    "test_result=clf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result.cache()\n",
    "#spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build evaluate metrics \n",
    "binary_eval=BinaryClassificationEvaluator()\n",
    "\n",
    "acc=binary_eval.evaluate(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under of ROC curve is 0.30.\n"
     ]
    }
   ],
   "source": [
    "print('The area under of ROC curve is {0:.2f}.'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Confusion matrix---------\n",
      "The true positive is 80954.\n",
      "The false positive is 93040.\n",
      "The false negative is 2631.\n",
      "The true negative is 109292.\n"
     ]
    }
   ],
   "source": [
    "tp=test_result[(test_result['label']==dict_encode['cat_dog'])& (test_result['prediction']==dict_encode['cat_dog'])].count()\n",
    "tn=test_result[(test_result['label']==dict_encode['other'])& (test_result['prediction']==dict_encode['other'])].count()\n",
    "fp=test_result[(test_result['label']==dict_encode['other'])& (test_result['prediction']==dict_encode['cat_dog'])].count()\n",
    "fn=test_result[(test_result['label']==dict_encode['cat_dog'])& (test_result['prediction']==dict_encode['other'])].count()\n",
    "\n",
    "print('--------Confusion matrix---------')\n",
    "print('The true positive is {}.'.format(tp))\n",
    "print('The false positive is {}.'.format(fp))\n",
    "print('The false negative is {}.'.format(fn))\n",
    "print('The true negative is {}.'.format(tn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Analysis----------\n",
      "The accuracy is 0.67.\n",
      "The true positive rate is 0.97\n",
      "The false positive rate is 0.46.\n",
      "The false negative rate is 0.03.\n"
     ]
    }
   ],
   "source": [
    "print('---------Analysis----------')\n",
    "\n",
    "print('The accuracy is {0:.2f}.'.format((tp+tn)/(tp+tn+fp+fn)))\n",
    "print('The true positive rate is {0:.2f}'.format(tp/(tp+fn)))\n",
    "print('The false positive rate is {0:.2f}.'.format(fp/(fp+tn)))\n",
    "print('The false negative rate is {0:.2f}.'.format(fn/(fn+tp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: i have applied Logistic and NaiveBayes model, The logistic regression have give area under ROC curve 0.91, while NaiveBayes give area under ROC curve 0.72. since process of labeling data is not accuracy. I prefer NB over logistic. Because 0.91 accuracy \n",
    "    is not reasonable based on  my label processing. The furture should build a more sophiscate way to label the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quesntion 3\n",
    "\n",
    "Appling classfier on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_frame.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_frame.registerTempTable('all_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_category=spark.sql(\"select *, \\\n",
    "#                  CONCAT(creator_name,' ',comment) as cnc from all_data\")\n",
    "all_category=new_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using build pipeline to process all data \n",
    "\n",
    "processed_all_data=processor.transform(all_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply classfier to all the data \n",
    "\n",
    "test_result=clf.transform(processed_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data=test_result.select('creator_name','userid','comment','prediction')\n",
    "all_data=test_result.select('userid','combined_comment','prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out category with cat/dog\n",
    "all_data_dog_cat=all_data.filter(all_data['prediction']==dict_encode['cat_dog'])\n",
    "all_data_other=all_data.filter(all_data['prediction']==dict_encode['other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 1348834 predicted dog/cat owner.\n"
     ]
    }
   ],
   "source": [
    "# while the true positive rate is 0.67. so the number of dog/cat owner is number of predicted dog/cat owner multiple \n",
    "# true positive rate \n",
    "dog_cat_count=all_data_dog_cat.count()\n",
    "print('There are total {} predicted dog/cat owner.'.format(dog_cat_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are total 2524849 userid\n",
    "\n",
    "Under this situation, the predicted cat/dog owner is 1348834 for traing data with 3:1 ratio (3 other vs 1 cat/dog owner).\n",
    "While for 1:1 ratio, the predicted cat/dog owner is about 1551838\n",
    "\n",
    "The above analysis are based on pre-labeled training data, which is not rigorous. This process could be furture proved by \n",
    "repeatly analyze the words set for pre-labeling traing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question4 : find topic of dog/cat owner\n",
    "  Using LDA model for topic analysis:\n",
    "  1, choose cat_dog owner \n",
    "  2, data preparing: strip black space, convert work to lower, remove none_letter word, remove word that we are not interested, like length of word smaller than 4 or 'that','this' et al.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all dog/cat comment to rdd\n",
    "comment=all_data_dog_cat.select('combined_comment').rdd\n",
    "\n",
    "comment_other=all_data_other.select('combined_comment').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean comment for topic analysis\n",
    "# set of word to remove before LDA analysis, collected after each LDA analysis\n",
    "stopword=['this','that','what','these','those','with','such','they','just',  \\\n",
    "         'would','them','your','from','have','thats','dont','thats','will',  \\\n",
    "         'really','there','about','please','more','much','still','being',   \\\n",
    "         'other','little','some','video','videos','could','wont','even','very',  \\\n",
    "         'back','even','always','like','love','good','nice','because','into',   \\\n",
    "         'were','their','is','are','great','then','when','been','know','want','think',  \\\n",
    "         'make','should']     \n",
    "\n",
    "def convert_line_to_Row(line):\n",
    "    tem={}\n",
    "    tem['combined_comment']=line\n",
    "    return tem\n",
    "# comment of dog and cat owner\n",
    "data0=comment.map(lambda line: line['combined_comment'].strip().lower()).  \\\n",
    "              map(lambda line: line.split()).                      \\\n",
    "              map(lambda line: [x for x in line if x.isalpha()]).  \\\n",
    "              map(lambda line: [x for x in line if len(x)>3]).     \\\n",
    "              map(lambda line: [x for x in line if x not in stopword]).    \\\n",
    "            filter(lambda line: line!=[]).map( lambda line: ' '.join(line)).  \\\n",
    "                map(lambda line : Row(**convert_line_to_Row(line)))\n",
    "\n",
    "                # comment of other user\n",
    "data0_other=comment_other.map(lambda line: line['combined_comment'].strip().lower()).  \\\n",
    "              map(lambda line: line.split()).                      \\\n",
    "              map(lambda line: [x for x in line if x.isalpha()]).  \\\n",
    "              map(lambda line: [x for x in line if len(x)>3]).     \\\n",
    "              map(lambda line: [x for x in line if x not in stopword]).    \\\n",
    "            filter(lambda line: line!=[]).map( lambda line: ' '.join(line)).  \\\n",
    "                map(lambda line : Row(**convert_line_to_Row(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert processed comment into dataframe\n",
    "data1=data0.toDF()\n",
    "\n",
    "data1_other=data0_other.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize processed comment \n",
    "token_cnc=Tokenizer(inputCol='combined_comment',outputCol='comment_token')\n",
    "data2=token_cnc.transform(data1)\n",
    "\n",
    "data2_other=token_cnc.transform(data1_other)\n",
    "# remove stop words\n",
    "stopremove=StopWordsRemover(inputCol='comment_token',outputCol='stop_remover')\n",
    "data3=stopremove.transform(data2)\n",
    "\n",
    "data3_other=stopremove.transform(data2_other)\n",
    "\n",
    "#convert them into term frquency \n",
    "countvect=CountVectorizer(inputCol='comment_token',outputCol='features')\n",
    "\n",
    "countmodel=countvect.fit(data3)\n",
    "data4=countmodel.transform(data3)\n",
    "\n",
    "countmodel_other=countvect.fit(data3_other)\n",
    "data4_other=countmodel_other.transform(data3_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter of LDA model\n",
    "number_topic=3\n",
    "max_number_iteration=20\n",
    "\n",
    "# Using lda model to train topic, assuming for 3 topic\n",
    "\n",
    "def print_topic(a,number_topic,tf_model):\n",
    "    '''\n",
    "    input: topics is the output of describeTopics()\n",
    "    '''\n",
    "\n",
    "    b=a.head(number_topic)\n",
    "    for i in range(number_topic):\n",
    "        topics1=[]\n",
    "        for j in b[i]['termIndices']:\n",
    "            topics1.append(tf_model.vocabulary[j])\n",
    "        print('The number {} topics: {}'.format(i+1,topics1))\n",
    "\n",
    "def lda_model(dataframe,number_topic,max_number_iteration,tf_model):\n",
    "\n",
    "    '''LDA model '''\n",
    "    ldf=LDA(k=number_topic,featuresCol='features',maxIter=max_number_iteration,seed=1)\n",
    "    ldf_model=ldf.fit(dataframe)\n",
    "\n",
    "    topics=ldf_model.describeTopics()\n",
    "\n",
    "    a=topics.select('termIndices')\n",
    "    print_topic(a,number_topic,tf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic for dog/cat owner\n",
      "The number 1 topics: ['people', 'animals', 'keep', 'thank', 'cute', 'time', 'hope', 'going', 'also', 'cant']\n",
      "The number 2 topics: ['fucking', 'fuck', 'jesus', 'cute', 'funny', 'shit', 'name', 'keep', 'call', 'ever']\n",
      "The number 3 topics: ['gohan', 'dogs', 'poor', 'husky', 'birthday', 'happy', 'name', 'kitty', 'people', 'sylvester']\n"
     ]
    }
   ],
   "source": [
    "# for dog/cat user \n",
    "print('Topic for dog/cat owner')\n",
    "lda_model(data4,number_topic,max_number_iteration,countmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dog/cat owner, the key words of topic 1 is about 'people', 'cute', it's more like praising. Topics 3 catched key words 'kitty' and 'husky'. I assume husky is a popular dog breed among dog users.  The key word 'birthday' was shown up with 'dogs'/'kitty'. The birthday Topic of these cat/dog owner indicated some presents, like toy, could be a good recommentation item each for creator of channel.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic for other owner\n",
      "The number 1 topics: ['coyote', 'name', 'cute', 'raptor', 'peterson', 'best', 'looks', 'stung', 'cool', 'channel']\n",
      "The number 2 topics: ['m·∫°nh', 'durian', 'clip', 'benim', 'gecko', 'ch∆°i', 'muhabbet', 'fruit', 'm√¨nh', 'shout']\n",
      "The number 3 topics: ['para', 'quiero', 'pero', 'como', 'esta', 'creo', 'tiene', 'funko', 'este', 'marvel']\n"
     ]
    }
   ],
   "source": [
    "print('Topic for other owner')\n",
    "lda_model(data4_other,number_topic,max_number_iteration,countmodel_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key word dog/cat do not show on topic of other user. The 'coyote','raptor' and 'stung' are definitely not words related with dog/cat topic. Topic 3 catched a quite few Spanish words. So number of Spanish user should be most beside English-Speaking user.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.registerTempTable('all_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+\n",
      "|   userid|    combined_comment|prediction|\n",
      "+---------+--------------------+----------+\n",
      "|2097155.0|subscribed he is ...|       0.0|\n",
      "|1048580.0|noooooooooooooooo...|       0.0|\n",
      "|      5.0|the christmas mus...|       0.0|\n",
      "|2097160.0|what do you do wi...|       0.0|\n",
      "|1048585.0|      –≤—ã –ª—É—á—å—à–µ –≤—Å–µ—Ö|       1.0|\n",
      "|     10.0|       i hope hes ok|       0.0|\n",
      "|2097165.0|made me go rewatc...|       0.0|\n",
      "|1048590.0|how many times di...|       0.0|\n",
      "|     15.0|i guess im a hedg...|       0.0|\n",
      "|2097170.0|this channel is s...|       0.0|\n",
      "|1048595.0|people are heartless|       0.0|\n",
      "|     20.0|explains my depre...|       0.0|\n",
      "|2097175.0|at you can see a ...|       0.0|\n",
      "|1048600.0|he speaks german ...|       0.0|\n",
      "|     25.0|                    |       1.0|\n",
      "|2097180.0|its never gonna h...|       0.0|\n",
      "|1048605.0| use your name brian|       0.0|\n",
      "|     30.0|                    |       1.0|\n",
      "|2097185.0|    what gun are you|       0.0|\n",
      "|1048610.0|my dick hurts whi...|       0.0|\n",
      "+---------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data=df_normal.toDF()\n",
    "normal_data.registerTempTable('normal_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data by creator_name, since cat_dog are labeled as 1.0, so sum(prediction) is number of total cat_dog owner for \n",
    "# each under each creator_name. and count(distinct_userid) are number of total userid. \n",
    "\n",
    "#ad=spark.sql('select creator_name, count(distinct userid) as total, sum(prediction) as cat_dog  \\\n",
    "#       from all_data group by creator_name')\n",
    "\n",
    "#ad=spark.sql('select n.creator_name, n.userid, a.prediction from normal_data as n inner join  \\\n",
    "#             all_data as a  on n.userid==a.userid')\n",
    "ad=spark.sql('select n.creator_name, n.userid, a.prediction from \\\n",
    "            (select creator_name, userid from normal_data group by creator_name, userid) as n inner join  \\\n",
    "            all_data as a  on n.userid==a.userid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ad.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.registerTempTable('ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad1=spark.sql('select creator_name, count(distinct userid) as total, (count(distinct userid)-sum(prediction)) as cat_dog  \\\n",
    "       from ad group by creator_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creator_name orderby  percentage of cat_dog owner\n",
    "# i filterd creator_name with more than 100 userid\n",
    "Percentage_of_user=ad1.withColumn('percentage',ad1['cat_dog']/ad1['total']).sort('percentage',ascending=False).  \\\n",
    "                                 filter(ad1['total']>50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[creator_name: string, total: bigint, cat_dog: double, percentage: double]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Percentage_of_user.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creator_name with number of userid more than 50\n",
      "+--------------------+-----+-------+------------------+\n",
      "|        creator_name|total|cat_dog|        percentage|\n",
      "+--------------------+-----+-------+------------------+\n",
      "|Dragon Tamer Rept...|   59|   59.0|               1.0|\n",
      "|       Day5 Aquatics|   92|   91.0|0.9891304347826086|\n",
      "|         Mike Hughes|   74|   73.0|0.9864864864864865|\n",
      "|    coloradoreinsman|   60|   59.0|0.9833333333333333|\n",
      "|       Brent Atwater|  263|  258.0|0.9809885931558935|\n",
      "|    Dr. Karen Becker|  132|  129.0|0.9772727272727273|\n",
      "|             Meldium|   84|   82.0|0.9761904761904762|\n",
      "|         Joey Ferris|  199|  194.0|0.9748743718592965|\n",
      "|    Maggie Krukowski|   68|   66.0|0.9705882352941176|\n",
      "|   Calm My Dog House|   99|   96.0|0.9696969696969697|\n",
      "|      Life With Dogs|   65|   63.0|0.9692307692307692|\n",
      "|Daves Pacific Nor...|  162|  157.0|0.9691358024691358|\n",
      "|           RatGirl44|  284|  275.0|0.9683098591549296|\n",
      "|Sanders Slitherin...|   63|   61.0|0.9682539682539683|\n",
      "|                 J B|   92|   89.0| 0.967391304347826|\n",
      "|Grace Hollow Rabb...|  122|  118.0|0.9672131147540983|\n",
      "|         wskrsnwings| 1174| 1135.0|0.9667802385008518|\n",
      "|     Kyles Fish room|  120|  116.0|0.9666666666666667|\n",
      "|   Infamous Aquatics|  148|  143.0|0.9662162162162162|\n",
      "|   Dee From Brooklyn|  188|  181.0|0.9627659574468085|\n",
      "+--------------------+-----+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "The top 50 creator_name are None.\n"
     ]
    }
   ],
   "source": [
    "# print top 10 creator_name with highest percentage of cat/dog owner\n",
    "\n",
    "number_rank=50\n",
    "print('Creator_name with number of userid more than 50')\n",
    "print('The top {} creator_name are {}.'.format(number_rank,Percentage_of_user.limit(number_rank).show()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment:\n",
    "    For creator_name have more than 50 userid. The creator_nane like 'ServiceDog Vlog','Calm My Dog House','Life With Dogs' et al are related with dog/cat literaly. There are a few creator_name with key word of 'fish' and 'aquatics' also were classified as dog/cat owner. I assume the comment of 'fish' and 'dog/cat' owner are sharing  some common feature or the cat/dog owner are also a fish owner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Percentage_of_user_100=ad1.withColumn('percentage',ad1['cat_dog']/ad1['total']).sort('percentage',ascending=False).  \\\n",
    "                                 filter(ad1['total']<=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creator_name with number of userid less than 50\n",
      "+--------------------+-----+-------+----------+\n",
      "|        creator_name|total|cat_dog|percentage|\n",
      "+--------------------+-----+-------+----------+\n",
      "|         Big Grizzly|    2|    2.0|       1.0|\n",
      "|          steve dale|    1|    1.0|       1.0|\n",
      "|          SDReptiles|    1|    1.0|       1.0|\n",
      "|                K945|    5|    5.0|       1.0|\n",
      "|Piano Tutorials b...|    8|    8.0|       1.0|\n",
      "|       MrAnimal Farm|    8|    8.0|       1.0|\n",
      "|      Labrador Daisy|    3|    3.0|       1.0|\n",
      "|          DogIDs.com|    4|    4.0|       1.0|\n",
      "|        AndrewConboy|    3|    3.0|       1.0|\n",
      "|        LeMasterReef|   13|   13.0|       1.0|\n",
      "|Aphmau X anime fo...|   11|   11.0|       1.0|\n",
      "|     Jonette Crowley|    6|    6.0|       1.0|\n",
      "|          nubianjuan|    1|    1.0|       1.0|\n",
      "|             CBleezy|    8|    8.0|       1.0|\n",
      "|   Best Bully Sticks|    2|    2.0|       1.0|\n",
      "|                Rach|    5|    5.0|       1.0|\n",
      "|     Georgia Trapper|    8|    8.0|       1.0|\n",
      "|       PixController|    1|    1.0|       1.0|\n",
      "|          Dr. Dragos|    4|    4.0|       1.0|\n",
      "|          Emily Pope|    1|    1.0|       1.0|\n",
      "+--------------------+-----+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "The top 50 creator_name are None.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "number_rank=50\n",
    "\n",
    "print('Creator_name with number of userid less than 50')\n",
    "print('The top {} creator_name are {}.'.format(number_rank,Percentage_of_user_100.limit(number_rank).show()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creator_name have less than 50 userid. It only have one creator_name 'DogIDs.com' related with dog/cat. While \n",
    "literally, it seems other creator_name are not related with dog/cat topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "end=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running time is 3.35 hours.\n"
     ]
    }
   ],
   "source": [
    "print('The running time is {0:.2f} hours.'.format((end-start)/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
